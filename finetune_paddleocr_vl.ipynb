{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PaddleOCR-VL Finetuning with Unsloth\n",
                "Finetuning on the `wrath/well-log-headers-ocr` dataset for well log OCR.\n",
                "\n",
                "This notebook uses the PaddleOCR-VL 1B model which works reliably with Unsloth's SFTTrainer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# !pip install uv && uv pip install unsloth\n",
                "# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install uv\n",
                "import os, re\n",
                "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
                "    !uv pip install unsloth\n",
                "else:\n",
                "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
                "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
                "    !uv pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
                "    !uv pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
                "    !uv pip install --no-deps unsloth\n",
                "!uv pip install transformers==4.56.2\n",
                "!uv pip install --no-deps trl==0.22.2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from unsloth import FastVisionModel\n",
                "import torch\n",
                "from transformers import AutoModelForCausalLM, AutoProcessor\n",
                "\n",
                "model_path = \"unsloth/PaddleOCR-VL\"\n",
                "\n",
                "model, tokenizer = FastVisionModel.from_pretrained(\n",
                "    # Model identifier from HuggingFace Hub.\n",
                "    # PaddleOCR-VL is a 1B parameter OCR-focused vision model.\n",
                "    # Works reliably with Unsloth's SFTTrainer unlike Qwen3-VL.\n",
                "    model_path,\n",
                "\n",
                "    # Maximum sequence length for input text + generated tokens.\n",
                "    # Unsloth recommends 2048 for vision tasks. Can increase to 4096+ for long outputs.\n",
                "    max_seq_length=2048,\n",
                "\n",
                "    # QLoRA 4-bit quantization. Reduces VRAM by ~75% with minimal quality loss.\n",
                "    # False = full precision (16-bit). PaddleOCR-VL uses full finetuning by default.\n",
                "    load_in_4bit=False,\n",
                "\n",
                "    # 8-bit quantization. Less memory savings than 4-bit but more accurate.\n",
                "    # Generally use either 4-bit OR 8-bit, not both.\n",
                "    load_in_8bit=False,\n",
                "\n",
                "    # Enable full finetuning of all parameters (not just LoRA adapters).\n",
                "    # Uses more VRAM but can achieve better results for domain-specific tasks.\n",
                "    full_finetuning=True,\n",
                "\n",
                "    # Base model class to use. AutoModelForCausalLM is standard for most VLMs.\n",
                "    auto_model=AutoModelForCausalLM,\n",
                "\n",
                "    # Required for models with custom code (like PaddleOCR-VL).\n",
                "    # WARNING: Only use with trusted models from HuggingFace.\n",
                "    trust_remote_code=True,\n",
                "\n",
                "    # Force Triton kernel compilation for faster inference.\n",
                "    # Unsloth recommendation: True for best performance.\n",
                "    unsloth_force_compile=True,\n",
                ")\n",
                "\n",
                "# Load processor separately - required for PaddleOCR-VL\n",
                "# The processor handles image preprocessing and tokenization.\n",
                "processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Apply LoRA Adapters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = FastVisionModel.get_peft_model(\n",
                "    model,\n",
                "\n",
                "    # LoRA rank. Higher = more parameters, more expressiveness, more VRAM.\n",
                "    # Unsloth recommends: 8-32 for small models, 64-128 for larger models.\n",
                "    # PaddleOCR notebook uses 64.\n",
                "    r=64,\n",
                "\n",
                "    # LoRA alpha scaling factor. Controls the magnitude of LoRA updates.\n",
                "    # Rule of thumb: set equal to r, or 2x r for stronger adaptation.\n",
                "    lora_alpha=64,\n",
                "\n",
                "    # Dropout probability for LoRA layers. 0 = no dropout (faster training).\n",
                "    # Unsloth recommends 0 for most cases.\n",
                "    lora_dropout=0,\n",
                "\n",
                "    # Whether to add bias to LoRA layers.\n",
                "    # \"none\" = no bias (Unsloth default, saves memory).\n",
                "    # \"all\" or \"lora_only\" = add bias.\n",
                "    bias=\"none\",\n",
                "\n",
                "    # Random seed for reproducibility.\n",
                "    # 3407 is Unsloth's \"lucky\" seed from their experiments.\n",
                "    random_state=3407,\n",
                "\n",
                "    # Rank-Stabilized LoRA. Improves training stability for high ranks.\n",
                "    # Unsloth recommends False for most cases.\n",
                "    use_rslora=False,\n",
                "\n",
                "    # Target modules for LoRA adaptation.\n",
                "    # These are the linear layers in the transformer that LoRA will modify.\n",
                "    # Includes attention (q/k/v/o_proj), MLP (gate/up/down_proj), and vision layers.\n",
                "    target_modules=[\n",
                "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention layers\n",
                "        \"gate_proj\", \"up_proj\", \"down_proj\",      # MLP layers\n",
                "        \"out_proj\", \"fc1\", \"fc2\",                 # Vision encoder layers\n",
                "        \"linear_1\", \"linear_2\"                    # Vision projector layers\n",
                "    ]\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Dataset (Lazy Loading for Memory Efficiency)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "from PIL import Image\n",
                "import base64\n",
                "from io import BytesIO\n",
                "import gc\n",
                "\n",
                "# Load raw dataset from HuggingFace\n",
                "raw_dataset = load_dataset(\"wrath/well-log-headers-ocr\")\n",
                "\n",
                "# Default instruction for OCR task\n",
                "default_instruction = \"\"\"Convert the following document to markdown.\n",
                "Return only the markdown with no explanation text. Do not include delimiters like ```markdown or ```html.\n",
                "\n",
                "RULES:\n",
                "- You must include all information on the page. Do not exclude headers, footers, or subtext.\n",
                "- Return tables in an HTML format.\n",
                "- Charts & infographics must be interpreted to a markdown format. Prefer table format when applicable.\n",
                "- Prefer using ‚òê and ‚òë for check boxes.\"\"\"\n",
                "\n",
                "def b64_to_image(b64_str):\n",
                "    \"\"\"Decode base64 string to PIL Image.\"\"\"\n",
                "    return Image.open(BytesIO(base64.b64decode(b64_str))).convert(\"RGB\")\n",
                "\n",
                "\n",
                "class LazyVisionDataset:\n",
                "    \"\"\"\n",
                "    MEMORY-EFFICIENT dataset wrapper with lazy image loading.\n",
                "    \n",
                "    Images are only decoded from base64 when accessed, preventing\n",
                "    RAM crashes from loading all images into memory at once.\n",
                "    This is critical for Colab's limited RAM (12-13GB).\n",
                "    \"\"\"\n",
                "    def __init__(self, hf_dataset, instruction=default_instruction):\n",
                "        self.data = hf_dataset\n",
                "        self.instruction = instruction\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.data)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        sample = self.data[idx]\n",
                "        img = b64_to_image(sample[\"image_base64\"])\n",
                "        return {\n",
                "            \"images\": [img],\n",
                "            \"messages\": [\n",
                "                {\"role\": \"user\", \"content\": [\n",
                "                    {\"type\": \"text\", \"text\": self.instruction},\n",
                "                    {\"type\": \"image\", \"image\": img}\n",
                "                ]},\n",
                "                {\"role\": \"assistant\", \"content\": [\n",
                "                    {\"type\": \"text\", \"text\": sample[\"answer\"]}\n",
                "                ]},\n",
                "            ]\n",
                "        }\n",
                "\n",
                "\n",
                "train_dataset = LazyVisionDataset(raw_dataset[\"train\"])\n",
                "eval_dataset = LazyVisionDataset(raw_dataset[\"eval\"])\n",
                "\n",
                "print(f\"Train: {len(train_dataset)} samples\")\n",
                "print(f\"Eval: {len(eval_dataset)} samples\")\n",
                "gc.collect()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Before Finetuning - Test Baseline on Eval Set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from IPython.display import display, HTML\n",
                "import random\n",
                "\n",
                "def pil_to_base64(img):\n",
                "    \"\"\"Convert PIL Image to base64 string for HTML embedding.\"\"\"\n",
                "    buffered = BytesIO()\n",
                "    img.save(buffered, format=\"PNG\")\n",
                "    return base64.b64encode(buffered.getvalue()).decode()\n",
                "\n",
                "def display_side_by_side(img, output, title=\"Sample\"):\n",
                "    \"\"\"Display image and rendered output side-by-side in notebook.\"\"\"\n",
                "    html = f\"\"\"\n",
                "    <div style=\"display:flex;gap:20px;padding:15px;background:#1e1e1e;border-radius:10px;margin:10px 0;\">\n",
                "        <div><h4 style=\"color:#888;\">{title}</h4>\n",
                "            <img src=\"data:image/png;base64,{pil_to_base64(img)}\" style=\"max-width:400px;border-radius:5px;\"/></div>\n",
                "        <div style=\"flex:1;background:white;color:black;padding:15px;border-radius:5px;overflow:auto;max-height:600px;\">\n",
                "            {output}</div>\n",
                "    </div>\"\"\"\n",
                "    display(HTML(html))\n",
                "\n",
                "random.seed(42)\n",
                "test_indices = random.sample(range(len(eval_dataset)), min(3, len(eval_dataset)))\n",
                "\n",
                "def run_inference(model, image, instruction=default_instruction):\n",
                "    \"\"\"\n",
                "    Run inference on a single image.\n",
                "    \n",
                "    Uses PaddleOCR-VL's recommended generation settings:\n",
                "    - temperature=1.5 for diverse outputs\n",
                "    - min_p=0.1 for quality filtering\n",
                "    \"\"\"\n",
                "    FastVisionModel.for_inference(model)\n",
                "    messages = [\n",
                "        {\"role\": \"user\", \"content\": [\n",
                "            {\"type\": \"image\"},\n",
                "            {\"type\": \"text\", \"text\": instruction}\n",
                "        ]}\n",
                "    ]\n",
                "    text_prompt = processor.tokenizer.apply_chat_template(\n",
                "        messages, tokenize=False, add_generation_prompt=True\n",
                "    )\n",
                "    inputs = processor(\n",
                "        image, text_prompt,\n",
                "        add_special_tokens=False,\n",
                "        return_tensors=\"pt\"\n",
                "    ).to(\"cuda\")\n",
                "    output = model.generate(\n",
                "        **inputs,\n",
                "        max_new_tokens=512,\n",
                "        use_cache=False,\n",
                "        temperature=1.5,\n",
                "        min_p=0.1\n",
                "    )\n",
                "    return processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
                "\n",
                "print(\"=== BEFORE FINETUNING (on EVAL set) ===\")\n",
                "baseline_outputs = []\n",
                "for i in test_indices:\n",
                "    sample = eval_dataset[i]\n",
                "    img = sample[\"images\"][0]\n",
                "    output = run_inference(model, img)\n",
                "    baseline_outputs.append(output)\n",
                "    display_side_by_side(img, output, f\"Eval Sample {i} - BEFORE\")\n",
                "    del sample\n",
                "gc.collect()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from trl import SFTTrainer, SFTConfig\n",
                "from unsloth.trainer import UnslothVisionDataCollator\n",
                "\n",
                "FastVisionModel.for_training(model)\n",
                "\n",
                "# UnslothVisionDataCollator - CRITICAL for vision finetuning!\n",
                "# This handles batching of multimodal (image + text) inputs correctly.\n",
                "custom_collator = UnslothVisionDataCollator(\n",
                "    # The model being trained\n",
                "    model=model,\n",
                "\n",
                "    # Processor for image/text preprocessing\n",
                "    processor=processor,\n",
                "\n",
                "    # Index to ignore in loss calculation (usually -100 for padding)\n",
                "    ignore_index=-100,\n",
                "\n",
                "    # Maximum sequence length for inputs\n",
                "    max_seq_length=2048,\n",
                "\n",
                "    # Only compute loss on assistant responses, not user prompts.\n",
                "    # This helps the model learn to generate outputs, not memorize inputs.\n",
                "    train_on_responses_only=True,\n",
                "\n",
                "    # String that marks the start of user instruction in the chat template.\n",
                "    # Must match the model's chat template format.\n",
                "    instruction_part=\"User: \",\n",
                "\n",
                "    # String that marks the start of assistant response in the chat template.\n",
                "    # Must match the model's chat template format.\n",
                "    response_part=\"\\nAssistant:\",\n",
                "\n",
                "    # Pad sequences to a multiple of this value for efficient GPU utilization.\n",
                "    # 8 is optimal for most GPUs.\n",
                "    pad_to_multiple_of=8,\n",
                ")\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    tokenizer=processor.tokenizer,\n",
                "    data_collator=custom_collator,\n",
                "    train_dataset=train_dataset,\n",
                "    args=SFTConfig(\n",
                "        # === BATCH SIZE ===\n",
                "        # Samples per GPU per forward pass. Higher = faster but more VRAM.\n",
                "        # Unsloth recommends 4 for T4 GPU with small models.\n",
                "        per_device_train_batch_size=4,\n",
                "\n",
                "        # Accumulate gradients over N steps before updating weights.\n",
                "        # Effective batch size = per_device_batch_size √ó gradient_accumulation_steps.\n",
                "        # Unsloth recommends 2-4 for vision tasks.\n",
                "        gradient_accumulation_steps=2,\n",
                "\n",
                "        # === TRAINING DURATION ===\n",
                "        # Number of warmup steps. Prevents early training instability.\n",
                "        # Rule of thumb: 5-10% of total steps, or 5 minimum.\n",
                "        warmup_steps=5,\n",
                "\n",
                "        # Total training steps. 60 for demo, 200-500+ for production.\n",
                "        # Alternatively, use num_train_epochs=1 for full dataset pass.\n",
                "        max_steps=60,\n",
                "\n",
                "        # === LEARNING RATE ===\n",
                "        # Learning rate for optimizer. Lower = more stable, slower convergence.\n",
                "        # Unsloth recommends 5e-5 for full finetuning, 2e-4 for LoRA.\n",
                "        learning_rate=5e-5,\n",
                "\n",
                "        # === LOGGING ===\n",
                "        # Log training metrics every N steps.\n",
                "        logging_steps=1,\n",
                "\n",
                "        # === OPTIMIZER ===\n",
                "        # 'adamw_8bit' saves ~30% memory vs standard AdamW.\n",
                "        # Unsloth recommendation for Colab.\n",
                "        optim=\"adamw_8bit\",\n",
                "\n",
                "        # L2 regularization to prevent overfitting. Range: 0.0 to 0.1.\n",
                "        weight_decay=0.001,\n",
                "\n",
                "        # Learning rate schedule. 'linear' decreases LR linearly to 0.\n",
                "        # Alternatives: 'cosine', 'constant', 'polynomial'.\n",
                "        lr_scheduler_type=\"linear\",\n",
                "\n",
                "        # === MISC ===\n",
                "        # Random seed for reproducibility. 3407 is Unsloth's \"lucky\" seed.\n",
                "        seed=3407,\n",
                "\n",
                "        # Directory to save checkpoints and logs.\n",
                "        output_dir=\"outputs\",\n",
                "\n",
                "        # Disable reporting to wandb/tensorboard. Use \"wandb\" to enable.\n",
                "        report_to=\"none\",\n",
                "\n",
                "        # === VISION-SPECIFIC (REQUIRED) ===\n",
                "        # Don't remove columns not in model signature (vision data has extra fields).\n",
                "        remove_unused_columns=False,\n",
                "\n",
                "        # Empty string required for vision tasks (we use custom collator).\n",
                "        dataset_text_field=\"\",\n",
                "\n",
                "        # Skip TRL's dataset preparation (we handle it ourselves).\n",
                "        dataset_kwargs={\"skip_prepare_dataset\": True},\n",
                "\n",
                "        # Maximum sequence length for model inputs.\n",
                "        max_length=2048,\n",
                "\n",
                "        # === PRECISION ===\n",
                "        # Use fp16 if bf16 not supported (older GPUs like T4).\n",
                "        fp16=not torch.cuda.is_bf16_supported(),\n",
                "\n",
                "        # Use bf16 if supported (A100, H100). Better training stability.\n",
                "        bf16=torch.cuda.is_bf16_supported(),\n",
                "    ),\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show current memory stats before training\n",
                "gpu_stats = torch.cuda.get_device_properties(0)\n",
                "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
                "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
                "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
                "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
                "\n",
                "# Train!\n",
                "trainer_stats = trainer.train()\n",
                "\n",
                "# Show final memory and time stats\n",
                "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
                "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
                "used_percentage = round(used_memory / max_memory * 100, 3)\n",
                "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
                "print(f\"\\n{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
                "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
                "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
                "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
                "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
                "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## After Finetuning - Compare Results on Eval Set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def display_comparison(img, before, after, expected, title):\n",
                "    \"\"\"Display 4-column comparison: image, before, after, expected.\"\"\"\n",
                "    html = f\"\"\"\n",
                "    <div style=\"padding:15px;background:#1e1e1e;border-radius:10px;margin:20px 0;\">\n",
                "        <h3 style=\"color:#4CAF50;\">{title}</h3>\n",
                "        <div style=\"display:flex;gap:10px;flex-wrap:wrap;\">\n",
                "            <div><h4 style=\"color:#888;\">Input Image</h4>\n",
                "                <img src=\"data:image/png;base64,{pil_to_base64(img)}\" style=\"max-width:200px;\"/></div>\n",
                "            <div style=\"flex:1;min-width:180px;\"><h4 style=\"color:#f44336;\">‚ùå Before</h4>\n",
                "                <div style=\"background:white;color:black;padding:8px;max-height:300px;overflow:auto;font-size:10px;\">{before[:1500]}</div></div>\n",
                "            <div style=\"flex:1;min-width:180px;\"><h4 style=\"color:#4CAF50;\">‚úÖ After</h4>\n",
                "                <div style=\"background:white;color:black;padding:8px;max-height:300px;overflow:auto;font-size:10px;\">{after[:1500]}</div></div>\n",
                "            <div style=\"flex:1;min-width:180px;\"><h4 style=\"color:#2196F3;\">üìã Expected</h4>\n",
                "                <div style=\"background:white;color:black;padding:8px;max-height:300px;overflow:auto;font-size:10px;\">{expected[:1500]}</div></div>\n",
                "        </div>\n",
                "    </div>\"\"\"\n",
                "    display(HTML(html))\n",
                "\n",
                "print(\"=== AFTER FINETUNING (on EVAL set - unseen data) ===\")\n",
                "for idx, i in enumerate(test_indices):\n",
                "    sample = eval_dataset[i]\n",
                "    img = sample[\"images\"][0]\n",
                "    expected = sample[\"messages\"][1][\"content\"][0][\"text\"]\n",
                "    after_output = run_inference(model, img)\n",
                "    display_comparison(img, baseline_outputs[idx], after_output, expected, f\"Eval Sample {i}\")\n",
                "    del sample\n",
                "gc.collect()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.save_pretrained(\"lora_model\")  # Local saving\n",
                "tokenizer.save_pretrained(\"lora_model\")\n",
                "print(\"LoRA adapters saved to ./lora_model\")\n",
                "\n",
                "# Optional: Push to HuggingFace Hub\n",
                "# model.push_to_hub(\"your_name/well-log-ocr-paddleocr\", token=\"...\")\n",
                "# tokenizer.push_to_hub(\"your_name/well-log-ocr-paddleocr\", token=\"...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Saved Model for Inference"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if False:  # Set to True to load saved model\n",
                "    from unsloth import FastVisionModel\n",
                "    model, tokenizer = FastVisionModel.from_pretrained(\n",
                "        model_name=\"lora_model\",  # Your saved model\n",
                "        load_in_4bit=False,\n",
                "    )\n",
                "    FastVisionModel.for_inference(model)\n",
                "\n",
                "    # Test inference\n",
                "    from transformers import TextStreamer\n",
                "    text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
                "    _ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128,\n",
                "                       use_cache=False, temperature=1.5, min_p=0.1)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
